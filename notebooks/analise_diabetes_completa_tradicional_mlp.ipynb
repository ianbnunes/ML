{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Comparativa: Algoritmos Tradicionais vs MLP (Redes Neurais)\n",
    "\n",
    "## Predição de Diabetes com Machine Learning e Deep Learning\n",
    "\n",
    "**Dataset:** Dataset_of_Diabetes.csv (1000 registros, 14 atributos)  \n",
    "**Objetivo:** Classificar pacientes em 3 categorias: Normal (N), Pré-diabético (P), Diabético (Y)  \n",
    "**Metodologia:** Comparação de 6 algoritmos tradicionais + 5 arquiteturas de Redes Neurais MLP\n",
    "\n",
    "---\n",
    "\n",
    "### Algoritmos Testados:\n",
    "\n",
    "**Algoritmos Tradicionais:**\n",
    "1. K-Nearest Neighbors (K-NN)\n",
    "2. Naive Bayes\n",
    "3. Support Vector Machine (SVM)\n",
    "4. Decision Tree\n",
    "5. Random Forest\n",
    "6. Logistic Regression\n",
    "\n",
    "**Arquiteturas MLP (Redes Neurais):**\n",
    "1. MLP Simples (1 camada: 50 neurônios)\n",
    "2. MLP Média (2 camadas: 100→50)\n",
    "3. MLP Profunda (3 camadas: 150→100→50)\n",
    "4. MLP Grande (2 camadas largas: 200→100)\n",
    "5. MLP Otimizada (Grid Search)\n",
    "\n",
    "**Total:** 11 modelos comparados!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports básicos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning - Algoritmos Tradicionais\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Deep Learning - Redes Neurais\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# MLflow e MinIO\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "print(\"✓ Todos os pacotes importados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do ambiente\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Diretório de saída\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"✓ Diretório criado: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"✓ Diretório já existe: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n✓ Ambiente configurado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuração MinIO e MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração MinIO\n",
    "MINIO_ENDPOINT = \"localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n",
    "MINIO_BUCKET = \"diabetes-analysis\"\n",
    "\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Criar bucket se não existir\n",
    "    if not minio_client.bucket_exists(MINIO_BUCKET):\n",
    "        minio_client.make_bucket(MINIO_BUCKET)\n",
    "        print(f\"✓ Bucket '{MINIO_BUCKET}' criado no MinIO\")\n",
    "    else:\n",
    "        print(f\"✓ Conectado ao bucket '{MINIO_BUCKET}' no MinIO\")\n",
    "    \n",
    "    MINIO_CONNECTED = True\n",
    "except Exception as e:\n",
    "    print(f\"  Erro ao conectar com MinIO: {e}\")\n",
    "    print(\"   Continuando sem MinIO...\")\n",
    "    MINIO_CONNECTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração MLflow\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "EXPERIMENT_NAME = \"Diabetes-Analysis-Traditional-vs-MLP\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"✓ MLflow configurado\")\n",
    "    print(f\"  • Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"  • Experiment: {EXPERIMENT_NAME}\")\n",
    "    MLFLOW_CONNECTED = True\n",
    "except Exception as e:\n",
    "    print(f\"  Erro ao configurar MLflow: {e}\")\n",
    "    print(\"   Continuando sem MLflow...\")\n",
    "    MLFLOW_CONNECTED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Carregamento dos Dados\n",
    "\n",
    "### Opção 1: PostgreSQL (se disponível)\n",
    "### Opção 2: CSV (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentar carregar do PostgreSQL\n",
    "POSTGRES_URL = \"postgresql+psycopg2://user:password@127.0.0.1:5432/diabetes_db\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(POSTGRES_URL)\n",
    "    raw_conn = engine.raw_connection()\n",
    "    \n",
    "    df = pd.read_sql(\"SELECT * FROM diabetes_tb\", raw_conn)\n",
    "    df.columns = [c.upper() for c in df.columns]\n",
    "    \n",
    "    print(f\"✓ Dados carregados do PostgreSQL: {df.shape[0]} registros\")\n",
    "    \n",
    "    raw_conn.close()\n",
    "    DATA_SOURCE = \"PostgreSQL\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  PostgreSQL não disponível: {e}\")\n",
    "    print(\"   Tentando carregar do CSV...\")\n",
    "    \n",
    "    # Fallback para CSV\n",
    "    try:\n",
    "        df = pd.read_csv('Dataset_of_Diabetes.csv')\n",
    "        print(f\"✓ Dados carregados do CSV: {df.shape[0]} registros\")\n",
    "        DATA_SOURCE = \"CSV\"\n",
    "    except:\n",
    "        print(\"   Erro: Arquivo CSV não encontrado!\")\n",
    "        print(\"   Coloque o arquivo 'Dataset_of_Diabetes.csv' no mesmo diretório do notebook.\")\n",
    "        raise FileNotFoundError(\"Dataset não encontrado\")\n",
    "\n",
    "# Mostrar informações do dataset\n",
    "print(f\"\\nInformações do Dataset:\")\n",
    "print(f\"  • Fonte: {DATA_SOURCE}\")\n",
    "print(f\"  • Dimensões: {df.shape[0]} registros × {df.shape[1]} atributos\")\n",
    "print(f\"  • Colunas: {df.columns.tolist()}\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  4. Análise Exploratória Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações básicas\n",
    "print(\"INFORMAÇÕES DO DATASET:\\n\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTATÍSTICAS DESCRITIVAS:\\n\")\n",
    "display(df.describe())\n",
    "\n",
    "# Distribuição das classes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISTRIBUIÇÃO DAS CLASSES:\\n\")\n",
    "df['CLASS'] = df['CLASS'].str.strip()\n",
    "class_dist = df['CLASS'].value_counts()\n",
    "print(class_dist)\n",
    "print()\n",
    "for classe, count in class_dist.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  • Classe '{classe}': {count} registros ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da distribuição das classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de barras\n",
    "class_dist.plot(kind='bar', ax=ax1, color=['green', 'orange', 'red'], edgecolor='black')\n",
    "ax1.set_title('Distribuição das Classes', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Classe', fontweight='bold')\n",
    "ax1.set_ylabel('Quantidade', fontweight='bold')\n",
    "ax1.set_xticklabels(['Diabético (Y)', 'Normal (N)', 'Pré-diabético (P)'], rotation=0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, v in enumerate(class_dist.values):\n",
    "    ax1.text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Gráfico de pizza\n",
    "colors = ['red', 'green', 'orange']\n",
    "ax2.pie(class_dist.values, labels=['Diabético (Y)', 'Normal (N)', 'Pré-diabético (P)'], \n",
    "        autopct='%1.1f%%', colors=colors, startangle=90, textprops={'fontweight': 'bold'})\n",
    "ax2.set_title('Proporção das Classes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/00_distribuicao_classes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráfico salvo: 00_distribuicao_classes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INICIANDO PRÉ-PROCESSAMENTO DOS DADOS...\\n\")\n",
    "\n",
    "# Identificar valores zeros (biologicamente impossíveis)\n",
    "cols_to_clean = ['Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI']\n",
    "\n",
    "print(\"Valores zeros encontrados (antes da limpeza):\")\n",
    "for col in cols_to_clean:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    if zero_count > 0:\n",
    "        percentage = (zero_count / len(df)) * 100\n",
    "        print(f\"  • {col}: {zero_count} valores ({percentage:.2f}%)\")\n",
    "\n",
    "# Substituir zeros por NaN\n",
    "df[cols_to_clean] = df[cols_to_clean].replace(0, np.nan)\n",
    "\n",
    "# Estratégia de imputação\n",
    "print(\"\\nAplicando estratégia de imputação:\")\n",
    "print(\"  • Mean: Urea, Cr, Chol, HDL (distribuições aprox. normais)\")\n",
    "print(\"  • Median: HbA1c, TG, LDL, VLDL, BMI (distribuições assimétricas)\")\n",
    "\n",
    "mean_cols = ['Urea', 'Cr', 'Chol', 'HDL']\n",
    "for col in mean_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "median_cols = ['HbA1c', 'TG', 'LDL', 'VLDL', 'BMI']\n",
    "for col in median_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"\\n✓ Valores ausentes tratados com sucesso!\")\n",
    "print(f\"  • Total de NaN após tratamento: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para modelagem\n",
    "print(\"\\nPREPARANDO DADOS PARA MODELAGEM...\\n\")\n",
    "\n",
    "# Remover colunas irrelevantes\n",
    "df_model = df.drop(['ID', 'No_Pation'], axis=1)\n",
    "print(f\"✓ Colunas ID e No_Pation removidas\")\n",
    "\n",
    "# Encoding de Gender\n",
    "label_encoder = LabelEncoder()\n",
    "df_model['Gender'] = label_encoder.fit_transform(df_model['Gender'])\n",
    "print(f\"✓ Gender codificado (F=0, M=1)\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df_model.drop('CLASS', axis=1)\n",
    "y = df_model['CLASS']\n",
    "\n",
    "print(f\"\\nDimensões finais:\")\n",
    "print(f\"  • Features (X): {X.shape}\")\n",
    "print(f\"  • Target (y): {y.shape}\")\n",
    "print(f\"  • Features: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"   DIVISÃO DOS DADOS (80/20):\\n\")\n",
    "print(f\"  • Treino: {X_train.shape[0]} registros\")\n",
    "print(f\"  • Teste: {X_test.shape[0]} registros\")\n",
    "print(f\"\\n  • Distribuição treino: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"  • Distribuição teste: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização (ESSENCIAL para MLP, KNN e SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"   NORMALIZAÇÃO CONCLUÍDA (StandardScaler)\\n\")\n",
    "print(f\"  • Mean ≈ 0, Std ≈ 1 para todas as features\")\n",
    "print(f\"\\n✓ Dados prontos para treinamento!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Treinamento - Algoritmos Tradicionais\n",
    "\n",
    "Testando 6 algoritmos clássicos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TREINAMENTO DOS ALGORITMOS TRADICIONAIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Definir algoritmos\n",
    "traditional_algorithms = {\n",
    "    'K-Nearest Neighbors (K-NN)': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Support Vector Machine (SVM)': SVC(kernel='linear', C=1.0, random_state=42, probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Armazenar resultados\n",
    "results = {}\n",
    "\n",
    "print(f\"\\n Treinando {len(traditional_algorithms)} algoritmos...\\n\")\n",
    "\n",
    "for idx, (name, model) in enumerate(traditional_algorithms.items(), 1):\n",
    "    print(f\"[{idx}/{len(traditional_algorithms)}] {name}...\", end=\" \")\n",
    "    \n",
    "    # Iniciar run do MLflow\n",
    "    if MLFLOW_CONNECTED:\n",
    "        with mlflow.start_run(run_name=f\"Traditional-{name}\"):\n",
    "            # Treinar modelo\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Log no MLflow\n",
    "            mlflow.log_param(\"algorithm_type\", \"Traditional\")\n",
    "            mlflow.log_param(\"algorithm_name\", name)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results[name] = {\n",
    "        'type': 'Tradicional',\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'confusion_matrix': cm,\n",
    "        'report': report\n",
    "    }\n",
    "    \n",
    "    print(f\"Acurácia: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\nAlgoritmos tradicionais treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  7. Treinamento - Redes Neurais MLP\n",
    "\n",
    "Testando 4 arquiteturas diferentes de MLP (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TREINAMENTO DAS REDES NEURAIS MLP\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Definir arquiteturas MLP\n",
    "mlp_configs = {\n",
    "    'MLP Simples (1 camada)': {\n",
    "        'hidden_layer_sizes': (50,),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'MLP Média (2 camadas)': {\n",
    "        'hidden_layer_sizes': (100, 50),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'MLP Profunda (3 camadas)': {\n",
    "        'hidden_layer_sizes': (150, 100, 50),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'MLP Grande (2 camadas largas)': {\n",
    "        'hidden_layer_sizes': (200, 100),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nTreinando {len(mlp_configs)} arquiteturas MLP...\\n\")\n",
    "\n",
    "for idx, (name, config) in enumerate(mlp_configs.items(), 1):\n",
    "    print(f\"[{idx}/{len(mlp_configs)}] {name}...\", end=\" \")\n",
    "    \n",
    "    # Criar modelo\n",
    "    mlp = MLPClassifier(**config, verbose=False)\n",
    "    \n",
    "    # Iniciar run do MLflow\n",
    "    if MLFLOW_CONNECTED:\n",
    "        with mlflow.start_run(run_name=f\"MLP-{name}\"):\n",
    "            # Treinar modelo\n",
    "            mlp.fit(X_train_scaled, y_train)\n",
    "            y_pred = mlp.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Log no MLflow\n",
    "            mlflow.log_param(\"algorithm_type\", \"MLP\")\n",
    "            mlflow.log_param(\"algorithm_name\", name)\n",
    "            mlflow.log_param(\"hidden_layers\", str(config['hidden_layer_sizes']))\n",
    "            mlflow.log_param(\"activation\", config['activation'])\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"iterations\", mlp.n_iter_)\n",
    "            mlflow.log_metric(\"final_loss\", mlp.loss_)\n",
    "            mlflow.sklearn.log_model(mlp, \"model\")\n",
    "    else:\n",
    "        mlp.fit(X_train_scaled, y_train)\n",
    "        y_pred = mlp.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results[name] = {\n",
    "        'type': 'MLP',\n",
    "        'model': mlp,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'confusion_matrix': cm,\n",
    "        'report': report,\n",
    "        'iterations': mlp.n_iter_,\n",
    "        'loss': mlp.loss_\n",
    "    }\n",
    "    \n",
    "    print(f\"Acurácia: {accuracy*100:.2f}% ({mlp.n_iter_} iterações)\")\n",
    "\n",
    "print(\"\\nRedes neurais treinadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Otimização com Grid Search (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\" OTIMIZAÇÃO DE HIPERPARÂMETROS (GRID SEARCH)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n⏳ Executando Grid Search...\")\n",
    "print(f\"   (Isso pode levar alguns minutos)\\n\")\n",
    "\n",
    "# Definir grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100, 50), (150, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "print(f\"   Configurações a testar: {len(param_grid['hidden_layer_sizes']) * len(param_grid['activation']) * len(param_grid['alpha']) * len(param_grid['learning_rate_init'])}\")\n",
    "print(f\"   • Hidden layers: {param_grid['hidden_layer_sizes']}\")\n",
    "print(f\"   • Activations: {param_grid['activation']}\")\n",
    "print(f\"   • Alpha (L2): {param_grid['alpha']}\")\n",
    "print(f\"   • Learning rate: {param_grid['learning_rate_init']}\")\n",
    "\n",
    "# Grid Search\n",
    "mlp_grid = MLPClassifier(max_iter=1000, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    mlp_grid, \n",
    "    param_grid, \n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n Grid Search concluído!\")\n",
    "print(f\"\\n  MELHORES HIPERPARÂMETROS:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "\n",
    "print(f\"\\n Melhor score (cross-validation): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Avaliar no teste\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred_best = best_mlp.predict(X_test_scaled)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"  Acurácia no teste: {accuracy_best*100:.2f}%\")\n",
    "\n",
    "# Log no MLflow\n",
    "if MLFLOW_CONNECTED:\n",
    "    with mlflow.start_run(run_name=\"MLP-Optimized-GridSearch\"):\n",
    "        mlflow.log_param(\"algorithm_type\", \"MLP\")\n",
    "        mlflow.log_param(\"algorithm_name\", \"MLP Otimizada (Grid Search)\")\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            mlflow.log_param(param, value)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy_best)\n",
    "        mlflow.log_metric(\"cv_score\", grid_search.best_score_)\n",
    "        mlflow.sklearn.log_model(best_mlp, \"model\")\n",
    "\n",
    "# Adicionar aos resultados\n",
    "results['MLP Otimizada (Grid Search)'] = {\n",
    "    'type': 'MLP',\n",
    "    'model': best_mlp,\n",
    "    'accuracy': accuracy_best,\n",
    "    'predictions': y_pred_best,\n",
    "    'confusion_matrix': confusion_matrix(y_test, y_pred_best),\n",
    "    'report': classification_report(y_test, y_pred_best, output_dict=True, zero_division=0),\n",
    "    'best_params': grid_search.best_params_\n",
    "}\n",
    "\n",
    "print(\"\\n Modelo otimizado salvo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  9. Análise Comparativa Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\" RANKING GERAL DE TODOS OS ALGORITMOS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Criar tabela comparativa\n",
    "comparison_data = []\n",
    "for name, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Algoritmo': name,\n",
    "        'Tipo': res['type'],\n",
    "        'Acurácia': res['accuracy'],\n",
    "        'Percentual': f\"{res['accuracy']*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Acurácia', ascending=False)\n",
    "comparison_df.index = range(1, len(comparison_df) + 1)\n",
    "\n",
    "print(f\"\\n{comparison_df[['Algoritmo', 'Tipo', 'Percentual']].to_string()}\")\n",
    "\n",
    "# Identificar melhores\n",
    "best_traditional = max([r for r in results.items() if r[1]['type'] == 'Tradicional'], \n",
    "                      key=lambda x: x[1]['accuracy'])\n",
    "best_mlp = max([r for r in results.items() if r[1]['type'] == 'MLP'], \n",
    "              key=lambda x: x[1]['accuracy'])\n",
    "best_overall = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"  MELHORES MODELOS POR CATEGORIA:\")\n",
    "print(f\"=\"*100)\n",
    "print(f\"\\n    Melhor Tradicional: {best_traditional[0]}\")\n",
    "print(f\"     Acurácia: {best_traditional[1]['accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n    Melhor MLP: {best_mlp[0]}\")\n",
    "print(f\"     Acurácia: {best_mlp[1]['accuracy']*100:.2f}%\")\n",
    "if 'iterations' in best_mlp[1]:\n",
    "    print(f\"     Iterações: {best_mlp[1]['iterations']}\")\n",
    "    print(f\"     Loss final: {best_mlp[1]['loss']:.6f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"   CAMPEÃO GERAL: {best_overall[0]}\")\n",
    "print(f\"   Acurácia: {best_overall[1]['accuracy']*100:.2f}%\")\n",
    "print(f\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar tabela comparativa\n",
    "comparison_df.to_csv(f'{OUTPUT_DIR}/comparacao_completa_todos_algoritmos.csv', index=False)\n",
    "print(f\"\\n✓ Tabela comparativa salva: comparacao_completa_todos_algoritmos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Visualizações Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 1: Comparação Geral\n",
    "print(\"  Gerando visualizações...\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Cores diferentes por tipo\n",
    "colors = []\n",
    "for tipo in comparison_df['Tipo']:\n",
    "    if tipo == 'Tradicional':\n",
    "        colors.append('steelblue')\n",
    "    else:\n",
    "        colors.append('darkorange')\n",
    "\n",
    "y_pos = range(len(comparison_df))\n",
    "bars = ax.barh(y_pos, comparison_df['Acurácia'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(comparison_df['Algoritmo'])\n",
    "ax.set_xlabel('Acurácia', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Comparação Geral: Algoritmos Tradicionais vs MLP (Redes Neurais)', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.set_xlim([0.9, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Adicionar valores\n",
    "for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "    ax.text(row['Acurácia'] + 0.002, i, f\"{row['Acurácia']:.2%}\", \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', edgecolor='black', label='Algoritmos Tradicionais'),\n",
    "    Patch(facecolor='darkorange', edgecolor='black', label='MLP (Redes Neurais)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/01_comparacao_geral_todos_algoritmos.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráfico salvo: 01_comparacao_geral_todos_algoritmos.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: Matrizes de Confusão - Top 6\n",
    "top_6 = comparison_df.head(6)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Matrizes de Confusão - Top 6 Modelos', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (_, row) in enumerate(top_6.iterrows()):\n",
    "    r = idx // 3\n",
    "    c = idx % 3\n",
    "    \n",
    "    name = row['Algoritmo']\n",
    "    result = results[name]\n",
    "    cm = result['confusion_matrix']\n",
    "    classes = sorted(y_test.unique())\n",
    "    \n",
    "    # Cor baseada no tipo\n",
    "    cmap = 'Blues' if result['type'] == 'Tradicional' else 'Oranges'\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                ax=axes[r, c], cbar_kws={'label': 'Contagem'})\n",
    "    \n",
    "    axes[r, c].set_title(f\"{name}\\n(Acc: {result['accuracy']:.2%})\", \n",
    "                        fontweight='bold', fontsize=10)\n",
    "    axes[r, c].set_xlabel('Predito', fontweight='bold')\n",
    "    axes[r, c].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/02_matrizes_confusao_top6.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráfico salvo: 02_matrizes_confusao_top6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 3: Convergência das MLPs\n",
    "mlp_models = [(name, res) for name, res in results.items() \n",
    "              if res['type'] == 'MLP' and 'best_params' not in res]\n",
    "\n",
    "if len(mlp_models) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Convergência dos Modelos MLP', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, (name, result) in enumerate(mlp_models[:4]):\n",
    "        r = idx // 2\n",
    "        c = idx % 2\n",
    "        \n",
    "        model = result['model']\n",
    "        if hasattr(model, 'loss_curve_'):\n",
    "            axes[r, c].plot(model.loss_curve_, 'darkorange', linewidth=2)\n",
    "            axes[r, c].set_xlabel('Épocas', fontweight='bold')\n",
    "            axes[r, c].set_ylabel('Loss', fontweight='bold')\n",
    "            axes[r, c].set_title(f'{name}\\n({result[\"iterations\"]} iterações, Loss final: {result[\"loss\"]:.6f})', \n",
    "                               fontsize=10, fontweight='bold')\n",
    "            axes[r, c].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/03_convergencia_mlps.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Gráfico salvo: 03_convergencia_mlps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Upload para MinIO (se disponível)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINIO_CONNECTED:\n",
    "    print(\"UPLOAD DE ARQUIVOS PARA MINIO\\n\")\n",
    "    \n",
    "    # Listar arquivos no diretório de outputs\n",
    "    files = [f for f in os.listdir(OUTPUT_DIR) if os.path.isfile(os.path.join(OUTPUT_DIR, f))]\n",
    "    \n",
    "    print(f\"Enviando {len(files)} arquivos para MinIO...\\n\")\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        object_name = f\"analysis/{datetime.now().strftime('%Y%m%d')}/{file}\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                MINIO_BUCKET,\n",
    "                object_name,\n",
    "                file_path\n",
    "            )\n",
    "            print(f\"  ✓ {file} → {object_name}\")\n",
    "        except S3Error as e:\n",
    "            print(f\"  ✗ Erro ao enviar {file}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Upload concluído! Arquivos disponíveis no bucket '{MINIO_BUCKET}'\")\n",
    "else:\n",
    "    print(\"MinIO não conectado. Arquivos salvos apenas localmente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Relatório Final Detalhado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"RELATÓRIO FINAL DETALHADO\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nMELHOR MODELO TRADICIONAL: {best_traditional[0]}\")\n",
    "print(f\"   Acurácia: {best_traditional[1]['accuracy']*100:.2f}%\\n\")\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, best_traditional[1]['predictions'], zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"MELHOR MODELO MLP: {best_mlp[0]}\")\n",
    "print(f\"   Acurácia: {best_mlp[1]['accuracy']*100:.2f}%\")\n",
    "if 'iterations' in best_mlp[1]:\n",
    "    print(f\"   Iterações: {best_mlp[1]['iterations']}\")\n",
    "    print(f\"   Loss final: {best_mlp[1]['loss']:.6f}\")\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, best_mlp[1]['predictions'], zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARAÇÃO: MELHOR TRADICIONAL vs MELHOR MLP\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "diff_pct = abs((best_traditional[1]['accuracy'] - best_mlp[1]['accuracy']) * 100)\n",
    "if best_traditional[1]['accuracy'] > best_mlp[1]['accuracy']:\n",
    "    print(f\"\\n✓ Algoritmo tradicional ({best_traditional[0]}) foi {diff_pct:.2f}% superior\")\n",
    "    print(f\"  Para este dataset tabular, algoritmos tradicionais são mais eficientes.\")\n",
    "else:\n",
    "    print(f\"\\n✓ MLP ({best_mlp[0]}) foi {diff_pct:.2f}% superior\")\n",
    "    print(f\"  Redes neurais capturaram padrões complexos eficientemente.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RECOMENDAÇÕES\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n1. Use {best_overall[0]} para máxima acurácia ({best_overall[1]['accuracy']*100:.2f}%)\")\n",
    "print(f\"2. Algoritmos tradicionais são mais simples e rápidos de treinar\")\n",
    "print(f\"3. MLPs são úteis para dados complexos ou não estruturados (imagens, texto)\")\n",
    "print(f\"4. Para datasets tabulares pequenos/médios, algoritmos tradicionais são suficientes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Salvar Resumo em Arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resumo completo\n",
    "with open(f'{OUTPUT_DIR}/resumo_completo_analise.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"ANÁLISE COMPARATIVA: ALGORITMOS TRADICIONAIS vs MLP (REDES NEURAIS)\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Dataset: {df.shape[0]} registros, {X.shape[1]} features\\n\")\n",
    "    f.write(f\"Data Source: {DATA_SOURCE}\\n\")\n",
    "    f.write(f\"Treino: {X_train.shape[0]} | Teste: {X_test.shape[0]}\\n\\n\")\n",
    "    \n",
    "    f.write(\"RANKING GERAL:\\n\")\n",
    "    f.write(\"-\" * 100 + \"\\n\")\n",
    "    for idx, (_, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "        f.write(f\"{idx}. {row['Algoritmo']:40s} ({row['Tipo']:12s}) - {row['Percentual']}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "    f.write(f\"CAMPEÃO GERAL: {best_overall[0]}\\n\")\n",
    "    f.write(f\"ACURÁCIA: {best_overall[1]['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "    f.write(\"COMPARAÇÃO: MELHOR TRADICIONAL vs MELHOR MLP\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(f\"\\nMelhor Tradicional: {best_traditional[0]}\\n\")\n",
    "    f.write(f\"Acurácia: {best_traditional[1]['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(f\"\\nMelhor MLP: {best_mlp[0]}\\n\")\n",
    "    f.write(f\"Acurácia: {best_mlp[1]['accuracy']*100:.2f}%\\n\")\n",
    "    \n",
    "    diff = (best_traditional[1]['accuracy'] - best_mlp[1]['accuracy']) * 100\n",
    "    if diff > 0:\n",
    "        f.write(f\"\\nDiferença: Tradicional é {diff:.2f}% melhor\\n\")\n",
    "    else:\n",
    "        f.write(f\"\\nDiferença: MLP é {-diff:.2f}% melhor\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "    f.write(\"CONEXÕES:\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(f\"\\nMLflow: {'Conectado ✓' if MLFLOW_CONNECTED else 'Não conectado ✗'}\\n\")\n",
    "    f.write(f\"MinIO: {'Conectado ✓' if MINIO_CONNECTED else 'Não conectado ✗'}\\n\")\n",
    "    f.write(f\"PostgreSQL: {'Conectado ✓' if DATA_SOURCE == 'PostgreSQL' else 'Não conectado ✗'}\\n\")\n",
    "\n",
    "print(\"✓ Resumo salvo: resumo_completo_analise.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ 14. Conclusão\n",
    "\n",
    "### Análise completa concluída com sucesso!\n",
    "\n",
    "**Foram testados:**\n",
    "- 6 algoritmos tradicionais de Machine Learning\n",
    "- 5 arquiteturas de Redes Neurais MLP\n",
    "- Otimização com Grid Search\n",
    "- Total: 11 modelos comparados\n",
    "\n",
    "**Integrações:**\n",
    "- MLflow para tracking de experimentos\n",
    "- MinIO para armazenamento de artefatos\n",
    "- PostgreSQL para dados (com fallback CSV)\n",
    "\n",
    "**Arquivos gerados:**\n",
    "- Gráficos comparativos\n",
    "- Matrizes de confusão\n",
    "- Tabelas de métricas\n",
    "- Relatórios detalhados\n",
    "\n",
    "---\n",
    "\n",
    "### Próximos passos sugeridos:\n",
    "1. Testar com validação cruzada k-fold\n",
    "2. Aplicar SMOTE para balancear classes minoritárias\n",
    "3. Testar ensemble de modelos\n",
    "4. Implementar feature engineering\n",
    "5. Deploy do melhor modelo em produção"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
